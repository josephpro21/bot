{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephpro21/bot/blob/main/engbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hmS3DuPbfY_"
      },
      "source": [
        "# Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srfh8C_iZ_jb",
        "outputId": "d198c641-8264-4287-f8f0-4f177e7a1d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "data_path = \"/content/drive/MyDrive/boteng\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH3vqlwme_r1"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO-TJDghe-e5",
        "outputId": "902a37c4-2b83-4c1c-d243-a127f5ceb235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh635SYvfygV"
      },
      "source": [
        "# Import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRAx7VSlf5vH",
        "outputId": "2bf6e178-3faa-405e-d8a1-81037b5ebef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'greating', 'patterns': ['Hi', 'Hey', 'Greatings', 'How are you'], 'responses': ['Am fine how about you', 'Fine how may i help you']}, {'tag': 'advice', 'patterns': ['how may i stay healthy', 'what health tips do you have for me', 'what should i eat during pregnancy'], 'responses': ['stay hydrated']}]}\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "data_files = glob.glob(data_path + \"/dataset.json\")\n",
        "\n",
        "with open(data_files[0], 'r') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "print(data)\n",
        "#for data_file in data_files:\n",
        "#  data = json.load(open(data_file))\n",
        "#  print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk9vYPOYg63b"
      },
      "source": [
        "# x and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-P0ndBcvg_-C"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "classes = []\n",
        "data_x = []\n",
        "data_y = []\n",
        "\n",
        "for intent in data[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    tokens = nltk.wordpunct_tokenize(pattern)\n",
        "    all_words.extend(tokens)\n",
        "    data_x.append(pattern)\n",
        "    data_y.append(intent[\"tag\"]),\n",
        "\n",
        "  if intent[\"tag\"] not in classes:\n",
        "    classes.append(intent[\"tag\"])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "all_words = [lemmatizer.lemmatize(word.lower()) for word in all_words if word not in string.punctuation]\n",
        "all_words = sorted(set(all_words))\n",
        "classes = sorted(set(classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ftfh77Kh8Ie"
      },
      "source": [
        "# Show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0-oYC9nh_kg",
        "outputId": "e5bc72db-cff6-4349-e40b-369737197472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "['Hi', 'Hey', 'Greatings', 'How are you', 'how may i stay healthy', 'what health tips do you have for me', 'what should i eat during pregnancy']\n",
            "['greating', 'greating', 'greating', 'greating', 'advice', 'advice', 'advice']\n",
            "['are', 'do', 'during', 'eat', 'for', 'greatings', 'have', 'health', 'healthy', 'hey', 'hi', 'how', 'i', 'may', 'me', 'pregnancy', 'should', 'stay', 'tip', 'what', 'you']\n"
          ]
        }
      ],
      "source": [
        "print(len(classes))\n",
        "print(data_x)\n",
        "print(data_y)\n",
        "print(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NbmDMfNMjQH9"
      },
      "outputs": [],
      "source": [
        "train = []\n",
        "output = [0] * len(classes)\n",
        "\n",
        "classes  = sorted(set(data_y))\n",
        "\n",
        "for idx, doc in enumerate(data_x):\n",
        "  bow = []\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "  for word in all_words:\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "  output_row = list(output)\n",
        "  output_row[classes.index(data_y[idx])] = 1\n",
        "  train.append([bow, output_row])\n",
        "\n",
        "random.shuffle(train)\n",
        "train = np.array(train, dtype=object)\n",
        "x_train = np.array(list(train[:, 0]))\n",
        "y_train = np.array(list(train[:, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYl8vBgGj_K6"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "rHLPyOZCkCJB",
        "outputId": "e1490aac-009d-458b-c741-adfe56ab2c25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adam = tf.keras.optimizers.Adam(learning_rate=0.01)\\ndef compile_model(loss,optimizer, metrics):\\n  model.compile(loss=loss,\\n                optimizer=optimizer,\\n                metrics=metrics)\\nmodel.compile(loss=\\'categorical_crossentropy\\',\\n              optimizer=adam,\\n              metrics=[\"accuracy\"])\\nprint(model.summary())\\nmodel.fit(x_train, y_train, epochs=32, batch_size=8, verbose=1)\\nmodel.save(\"/content/model/chatbot_model.h5\")\\nmodel.evaluate(x_train, y_train)\\nprint(\"Done\") \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#model = Sequential()\n",
        "#model.add(Dense(128, input_shape=(len(x_train[0]),), activation=\"relu\"))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(64, activation=\"relu\"))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(len(y_train[0]), activation=\"softmax\"))\n",
        "'''adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "def compile_model(loss,optimizer, metrics):\n",
        "  model.compile(loss=loss,\n",
        "                optimizer=optimizer,\n",
        "                metrics=metrics)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "model.fit(x_train, y_train, epochs=32, batch_size=8, verbose=1)\n",
        "model.save(\"/content/model/chatbot_model.h5\")\n",
        "model.evaluate(x_train, y_train)\n",
        "print(\"Done\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(\"/content/model/chatbot_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ug4b4MeDQSn",
        "outputId": "c97c9ddd-1f35-4ed3-8296-0f2fa594c447"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b00eRGL1mYSX"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  tokens = nltk.wordpunct_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "def bag_of_words(text, vocab):\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bow[idx] = 1\n",
        "  return np.array(bow)\n",
        "def predict_class(text, vocab, labels):\n",
        "  bow = bag_of_words(text, vocab)\n",
        "  result = model.predict(np.array([bow]))[0]\n",
        "  thresh = 0.2\n",
        "  y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in y_pred:\n",
        "    return_list.append({'intent': labels[r[0]], 'probability': r[1]})\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  if len(intents_list) == 0:\n",
        "    result = \"Sorry! I don't Comprehende.\"\n",
        "  else:\n",
        "    tag = intents_list[0]\n",
        "    list_of_intents = intents_json[\"intents\"]\n",
        "    for i in list_of_intents:\n",
        "      if i[\"tag\"] == tag:\n",
        "        result = random.choice(i[\"responses\"])\n",
        "        break\n",
        "      else:\n",
        "        result = \"Sorry! I don't Comprehende.\"\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O1hCn-vpnOJ6"
      },
      "outputs": [],
      "source": [
        "def pred_class(sentence,words,classes):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "  bag = [1 if word in all_words else 0 for word in sentence_words]\n",
        "  return np.array(bag)\n",
        "\n",
        "def getting_response(intents_list, data):\n",
        "  # Check if intents_list is empty to avoid IndexError\n",
        "  if not intents_list:\n",
        "    return \"Sorry! I don't Comprehende.\"\n",
        "\n",
        "  # intents_list[0] is already a dictionary with 'intent' as a key\n",
        "  tag = intents_list[0]['intent']\n",
        "\n",
        "  list_of_intents = data[\"intents\"]\n",
        "  result = \"\"\n",
        "  for i in list_of_intents:\n",
        "    if i[\"tag\"] == tag:  # Compare with 'tag' in your data\n",
        "      result = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y58WnIhsobgr",
        "outputId": "ff5a1ad6-f855-41e4-8253-ea91102e8493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Press 7 to quit the bot\n",
            "Enter Prompt: \thello\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "Am fine how about you\n",
            "Enter Prompt: \t7\n"
          ]
        }
      ],
      "source": [
        "print(\"Press 7 to quit the bot\")\n",
        "while True:\n",
        "  prompt = input(\"Enter Prompt: \\t\")\n",
        "  if prompt == \"7\":\n",
        "    break\n",
        "  intents = predict_class(prompt, all_words, classes)\n",
        "  result = getting_response(intents, data)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHqQHcNcQkuL"
      },
      "source": [
        "# API"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import tensorflow as tf\n",
        "loaded_model = model\n",
        "metrics = loaded_model.evaluate(x_train, y_train)\n",
        "loss = metrics[0]*100\n",
        "accuracy = metrics[1]*100\n",
        "print(\"Loss:\", metrics[0]*100)\n",
        "print(\"Accuracy:\", metrics[1]*100)\n",
        "app = Flask(__name__)\n",
        "@app.route('/prediction', methods = ['POST'])\n",
        "def prediction():\n",
        "  prompt = request.get_json()\n",
        "  user_prompt = prompt['prompt']\n",
        "  intents = predict_class(user_prompt, all_words, classes)\n",
        "  result = getting_response(intents,data)\n",
        "  return jsonify({\"response\": result, \"accuracy\": accuracy, \"loss\": loss})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.run()"
      ],
      "metadata": {
        "id": "4T4XUUL7ATCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wBEE4ccQpL-",
        "outputId": "e9bbb735-c787-47ff-cb13-6627f5504e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n",
            "3.1.0\n",
            "3.9.1\n",
            "1.26.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ab0be2592e48>:6: DeprecationWarning: The '__version__' attribute is deprecated and will be removed in Flask 3.1. Use feature detection or 'importlib.metadata.version(\"flask\")' instead.\n",
            "  print(flask.__version__)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "import flask\n",
        "import nltk\n",
        "import numpy as np\n",
        "print(tensorflow.__version__)\n",
        "print(flask.__version__)\n",
        "print(nltk.__version__)\n",
        "print(np.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_1SH5yFfUXW",
        "outputId": "0744d306-3cda-421d-8197-271eaa770093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 1.0000 - loss: 6.1477e-06\n",
            "Loss: 0.0006147718067950336\n",
            "Accuracy: 100.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRCKv3Pg7eUeiGWxMpoDTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}